{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Hyerparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken as ttk\n",
    "from torch.nn import functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "context_length = 4 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "torch.set_default_device(\"cuda\") if torch.cuda.is_available() else torch.set_default_device(\"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Modular Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward():\n",
    "        pass\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = torch.nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = torch.nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = torch.nn.ModuleList([EncoderLayer(d_model, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = torch.nn.ModuleList([DecoderLayer(d_model, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = torch.nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_289413/461433027.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(\"data.pt\")\n"
     ]
    }
   ],
   "source": [
    "def process_input():\n",
    "\n",
    "    ds = load_dataset(\"milkshake721/2.1M-wiki-STEM\", split=\"train\") \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(batch[\"text\"], padding=False, truncation=False)\n",
    "\n",
    "    tokenized_ds = ds.map(tokenize_batch, batched=True, num_proc=4)\n",
    "    flat_encoded = [token for example in tokenized_ds[\"input_ids\"] for token in example]\n",
    "    # im also thinking that maybe the length of the encoded list is good? maybe?\n",
    "    n_vocab = len(set(flat_encoded))\n",
    "    return n_vocab, torch.tensor(flat_encoded, dtype=torch.long)\n",
    "    # looking at the tokenized output will essentially give us a \"one to one\" translation of the text\n",
    "\n",
    "def build_cbow_pairs(data, context_size=2):\n",
    "    for i in range(context_size, len(data) - context_size):\n",
    "        left = data[i - context_size:i]\n",
    "        right = data[i + 1:i + context_size + 1]\n",
    "        context = torch.cat((left, right))\n",
    "        center = data[i]\n",
    "        yield context, center\n",
    "\n",
    "if os.path.exists(\"./data.pt\"):\n",
    "    print(\"here\")\n",
    "    data = torch.load(\"data.pt\")\n",
    "    vocab_size = int(torch.max(data)) + 1\n",
    "else:\n",
    "    vocab_size, data = process_input()\n",
    "    torch.save(data, \"data.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_token_size, d_model, context_length, data):\n",
    "        super().__init__()\n",
    "\n",
    "        # define some important vars\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        self.d_model = d_model\n",
    "        self.input_size = input_token_size\n",
    "\n",
    "        self.token_embedding_table = torch.nn.Embedding(\n",
    "            vocab_size + 1, d_model, \n",
    "        )\n",
    "        self.linear_one = torch.nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds: torch.Tensor = self.token_embedding_table(x)\n",
    "        input_embeds = embeds.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        out: torch.Tensor = self.linear_one(input_embeds)\n",
    "        return out.squeeze(0)\n",
    "\n",
    "    # def generate(self, x, max_new_tokens):\n",
    "    #     for _ in range(max_new_tokens):\n",
    "    #         # get the predictions\n",
    "    #         logits, _ = self.forward(x)\n",
    "    #         # focus only on the last time step\n",
    "    #         logits = logits[:, -1, :]  # becomes (B, C)\n",
    "    #         # apply softmax to get probabilities\n",
    "    #         probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "    #         # sample from the distribution\n",
    "    #         idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "    #         # append sampled index to the running sequence\n",
    "    #         x = torch.cat((x, idx_next), dim=1)  # (B, T+1)\n",
    "    #     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_model = 8\n",
    "embedding_layer = EmbeddingLayer(vocab_size, vocab_size, d_model, context_length, data)\n",
    "# embedding_layer.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = build_cbow_pairs(data, context_length)\n",
    "sum(1 for _ in pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000, Avg loss (last 1000): 9.1025\n",
      "Step 2000, Avg loss (last 1000): 8.5620\n",
      "Step 3000, Avg loss (last 1000): 9.2192\n",
      "Step 4000, Avg loss (last 1000): 10.0647\n",
      "Step 5000, Avg loss (last 1000): 10.0152\n",
      "Step 6000, Avg loss (last 1000): 9.9115\n",
      "Step 7000, Avg loss (last 1000): 10.4646\n",
      "Step 8000, Avg loss (last 1000): 10.8048\n",
      "Step 9000, Avg loss (last 1000): 10.4969\n",
      "Step 10000, Avg loss (last 1000): 10.2949\n",
      "Step 11000, Avg loss (last 1000): 10.3838\n",
      "Step 12000, Avg loss (last 1000): 10.5647\n",
      "Step 13000, Avg loss (last 1000): 11.6084\n",
      "Step 14000, Avg loss (last 1000): 11.5481\n",
      "Step 15000, Avg loss (last 1000): 10.6677\n",
      "Step 16000, Avg loss (last 1000): 10.2573\n",
      "Step 17000, Avg loss (last 1000): 10.3674\n",
      "Step 18000, Avg loss (last 1000): 10.2805\n",
      "Step 19000, Avg loss (last 1000): 10.1444\n",
      "Step 20000, Avg loss (last 1000): 9.6919\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=1e-3)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# train the model\n",
    "for epoch in range(1):\n",
    "    \n",
    "    step = 0\n",
    "    log_loss = 0\n",
    "    pairs = build_cbow_pairs(data, context_length)\n",
    "    # print(\"Steps: \", sum(1 for _ in pairs))\n",
    "    for context, target in pairs:\n",
    "        # print(context, \"->\", target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = embedding_layer(context)\n",
    "\n",
    "        # print(f\"{logits=}\\n{logits.size()=}\\n\")\n",
    "        # print(f\"{target=}\\n{target.size()=}\\n\")\n",
    "        \n",
    "        loss = loss_fn(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        log_loss += loss.item()\n",
    "\n",
    "        if step % 1000 == 0 and step > 0:\n",
    "            print(f\"Step {step}, Avg loss (last {1000}): {log_loss / 1000:.4f}\")\n",
    "            losses.append(log_loss)\n",
    "            log_loss = 0\n",
    "        step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(losses))], losses, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage loss per 1000 steps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss (Cross-Entropy)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([i for i in range(len(losses))], losses, marker='o', linestyle='-')\n",
    "plt.xlabel('Average loss per 1000 steps')\n",
    "plt.ylabel('Loss (Cross-Entropy)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
