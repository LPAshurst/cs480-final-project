{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken as ttk\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Modular Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2778419223.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class AttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward():\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTransformer\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_vocab_size, tgt_vocab_size, d_model, num_layers, d_ff, max_seq_length, dropout):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Transformer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = torch.nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = torch.nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = torch.nn.ModuleList([EncoderLayer(d_model, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = torch.nn.ModuleList([DecoderLayer(d_model, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = torch.nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_token_size, embedding_dimensions, data):\n",
    "        super().__init__()\n",
    "\n",
    "        # define some important vars\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        self.emb_d = embedding_dimensions\n",
    "        self.input_size = input_token_size\n",
    "        \n",
    "        print(\"Input Size: \", self.input_size)\n",
    "        print(\"Emb Size: \", self.emb_d)\n",
    "\n",
    "        self.token_embedding_table = torch.nn.Embedding(\n",
    "            self.vocab_size, 2\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y: torch.Tensor = None):\n",
    "        print(\"Beginning of Forward\")\n",
    "        print(f\"{x=}\")\n",
    "        print(f\"{y=}\\n\")\n",
    "        \n",
    "        logits: torch.Tensor = self.token_embedding_table(x)\n",
    "        print(f\"{logits.shape=}\")\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # logits becomesa tensor of size (Batch size, Sequence Length (T), vocab_size)\n",
    "            B, T, C = logits.shape  # (Batch size, Sequence Length (T), vocab_size)\n",
    "            logits = logits.view(\n",
    "                B * T, C\n",
    "            )  # reshape the logits so they can be used in cross entropy loss\n",
    "            print(f\"{y=}\")\n",
    "            print(f\"{y.shape=}\")\n",
    "            print(f\"{type(y)=}\")\n",
    "            targets = y.view(B * T)\n",
    "            print(f\"{logits.shape=} {logits=}\")\n",
    "            print(f\"{targets.shape=} {targets=}\")   \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self.forward(x)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            x = torch.cat((x, idx_next), dim=1)  # (B, T+1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # this might be reflective of the encoder model but for right now i dont actually know\n",
    "        # vocab_size = len(set(text))\n",
    "\n",
    "        # tokenize with byte pair encoding.\n",
    "        # This gives us a shorter token array lenght becuase we arent splitting by character\n",
    "        enc = ttk.get_encoding(\"gpt2\")\n",
    "        encoded = enc.encode(text)\n",
    "        print(f\"Text: {list(text.split())[:5]}: Length: {len(text.split())}\")\n",
    "        print(f\"Encoded: {encoded[:5]}: Length: {len(encoded)}\")\n",
    "        return enc.n_vocab, torch.tensor(encoded, dtype=torch.long)\n",
    "        # looking at the tokenized output will essentially give us a \"one to one\" translation of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, train_data, val_data, block_size, batch_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['First', 'Citizen:', 'Before', 'we', 'proceed']: Length: 155183\n",
      "Encoded: [5962, 22307, 25, 198, 8421]: Length: 255888\n",
      "vocab_size=50257\n",
      "tensor([ 5962, 22307,    25,  ...,    33,  1094, 42391])\n",
      "255888\n"
     ]
    }
   ],
   "source": [
    "vocab_size, data = process_input(\"input.txt\")\n",
    "print(f\"{vocab_size=}\")\n",
    "print(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:  255888\n",
      "Emb Size:  10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "# block size and batch size can change\n",
    "xb, yb = get_batch(\"train\", train_data, val_data, 10, 5)\n",
    "\n",
    "embedding_layer = EmbeddingLayer(vocab_size, len(data), 10, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  284,   345,    11,   198,  8496,   339,   815,  1064,   345, 31420],\n",
      "        [ 7597,  3963, 28154,    25,   198,    39,   451,   502,   257,  1573],\n",
      "        [  618,   673, 10564,   351,  8737, 10564,   607,  3650,    13,   198],\n",
      "        [ 8643,  3528,  1340,  2937,    25,   198, 16773,    11,  3595, 42666],\n",
      "        [ 3398,  7597,  3963, 28154,    25,   198,  5195,    11,   644,   318]])\n",
      "tensor([[  345,    11,   198,  8496,   339,   815,  1064,   345, 31420,    11],\n",
      "        [ 3963, 28154,    25,   198,    39,   451,   502,   257,  1573,    26],\n",
      "        [  673, 10564,   351,  8737, 10564,   607,  3650,    13,   198,   198],\n",
      "        [ 3528,  1340,  2937,    25,   198, 16773,    11,  3595, 42666,    25],\n",
      "        [ 7597,  3963, 28154,    25,   198,  5195,    11,   644,   318,   340]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)\n",
    "print(yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of Forward\n",
      "x=tensor([[  284,   345,    11,   198,  8496,   339,   815,  1064,   345, 31420],\n",
      "        [ 7597,  3963, 28154,    25,   198,    39,   451,   502,   257,  1573],\n",
      "        [  618,   673, 10564,   351,  8737, 10564,   607,  3650,    13,   198],\n",
      "        [ 8643,  3528,  1340,  2937,    25,   198, 16773,    11,  3595, 42666],\n",
      "        [ 3398,  7597,  3963, 28154,    25,   198,  5195,    11,   644,   318]])\n",
      "y=tensor([[  345,    11,   198,  8496,   339,   815,  1064,   345, 31420,    11],\n",
      "        [ 3963, 28154,    25,   198,    39,   451,   502,   257,  1573,    26],\n",
      "        [  673, 10564,   351,  8737, 10564,   607,  3650,    13,   198,   198],\n",
      "        [ 3528,  1340,  2937,    25,   198, 16773,    11,  3595, 42666,    25],\n",
      "        [ 7597,  3963, 28154,    25,   198,  5195,    11,   644,   318,   340]])\n",
      "\n",
      "logits.shape=torch.Size([5, 10, 2])\n",
      "y=tensor([[  345,    11,   198,  8496,   339,   815,  1064,   345, 31420,    11],\n",
      "        [ 3963, 28154,    25,   198,    39,   451,   502,   257,  1573,    26],\n",
      "        [  673, 10564,   351,  8737, 10564,   607,  3650,    13,   198,   198],\n",
      "        [ 3528,  1340,  2937,    25,   198, 16773,    11,  3595, 42666,    25],\n",
      "        [ 7597,  3963, 28154,    25,   198,  5195,    11,   644,   318,   340]])\n",
      "y.shape=torch.Size([5, 10])\n",
      "type(y)=<class 'torch.Tensor'>\n",
      "logits.shape=torch.Size([50, 2]) logits=tensor([[-1.0486, -0.9066],\n",
      "        [-1.7579, -0.5128],\n",
      "        [-1.0370, -1.4516],\n",
      "        [ 1.6549,  1.2960],\n",
      "        [-1.2645, -0.4407],\n",
      "        [ 0.5295, -0.6723],\n",
      "        [-0.6149, -0.5337],\n",
      "        [-1.3368,  0.6766],\n",
      "        [-1.7579, -0.5128],\n",
      "        [-1.5029,  0.3220],\n",
      "        [-0.3427,  0.2110],\n",
      "        [ 0.2871, -2.6025],\n",
      "        [-1.1577, -1.8615],\n",
      "        [ 1.7411, -0.3124],\n",
      "        [ 1.6549,  1.2960],\n",
      "        [ 1.4535,  0.8414],\n",
      "        [-0.2780, -0.0913],\n",
      "        [ 0.2272,  2.4243],\n",
      "        [ 1.0051, -1.7815],\n",
      "        [ 0.3587, -1.2105],\n",
      "        [ 0.1859,  0.6091],\n",
      "        [ 1.5276,  0.3667],\n",
      "        [ 2.3021,  0.0301],\n",
      "        [-0.0108,  0.7268],\n",
      "        [ 1.7225, -1.3836],\n",
      "        [ 2.3021,  0.0301],\n",
      "        [-0.4314,  0.3922],\n",
      "        [-2.2229,  0.5244],\n",
      "        [ 0.4160, -1.2997],\n",
      "        [ 1.6549,  1.2960],\n",
      "        [-0.1287,  1.3544],\n",
      "        [ 1.3101,  0.9350],\n",
      "        [ 1.3052,  0.2221],\n",
      "        [ 0.4932,  0.8355],\n",
      "        [ 1.7411, -0.3124],\n",
      "        [ 1.6549,  1.2960],\n",
      "        [ 0.4557,  0.2386],\n",
      "        [-1.0370, -1.4516],\n",
      "        [-0.4778,  0.4980],\n",
      "        [-0.4970, -0.4619],\n",
      "        [ 0.8925,  0.6435],\n",
      "        [-0.3427,  0.2110],\n",
      "        [ 0.2871, -2.6025],\n",
      "        [-1.1577, -1.8615],\n",
      "        [ 1.7411, -0.3124],\n",
      "        [ 1.6549,  1.2960],\n",
      "        [-1.5201, -0.8680],\n",
      "        [-1.0370, -1.4516],\n",
      "        [ 2.2523, -1.0234],\n",
      "        [-0.3468,  0.5059]], grad_fn=<ViewBackward0>)\n",
      "targets.shape=torch.Size([50]) targets=tensor([  345,    11,   198,  8496,   339,   815,  1064,   345, 31420,    11,\n",
      "         3963, 28154,    25,   198,    39,   451,   502,   257,  1573,    26,\n",
      "          673, 10564,   351,  8737, 10564,   607,  3650,    13,   198,   198,\n",
      "         3528,  1340,  2937,    25,   198, 16773,    11,  3595, 42666,    25,\n",
      "         7597,  3963, 28154,    25,   198,  5195,    11,   644,   318,   340])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 345 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m embedding_layer\u001b[38;5;241m.\u001b[39mforward(xb, yb)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m      4\u001b[0m enc \u001b[38;5;241m=\u001b[39m ttk\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[36], line 39\u001b[0m, in \u001b[0;36mEmbeddingLayer.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargets\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargets\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)   \n\u001b[0;32m---> 39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, targets)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.12/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 345 is out of bounds."
     ]
    }
   ],
   "source": [
    "_, loss = embedding_layer.forward(xb, yb)\n",
    "print(loss)\n",
    "\n",
    "enc = ttk.get_encoding(\"gpt2\")\n",
    "\n",
    "decoded = enc.decode(\n",
    "    embedding_layer.generate(\n",
    "        torch.zeros(1, 1, dtype=torch.long), max_new_tokens=100\n",
    "    )[0].tolist()\n",
    ")\n",
    "# print(f\"{decoded:}\")\n",
    "# could do SGD but whatever\n",
    "optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=1e-3)\n",
    "\n",
    "# train the model\n",
    "for steps in range(100):\n",
    "    xb, yb = get_batch(\"train\", train_data, val_data, 10, 5)\n",
    "    logits, loss = embedding_layer.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
