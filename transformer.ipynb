{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken as ttk\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_token_size, embedding_dimensions, data):\n",
    "        super().__init__()\n",
    "\n",
    "        # define some important vars\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        self.emb_d = embedding_dimensions\n",
    "        self.input_size = input_token_size\n",
    "        \n",
    "        print(\"Input Size: \", self.input_size)\n",
    "        print(\"Emb Size: \", self.emb_d)\n",
    "\n",
    "        self.token_embedding_table = torch.nn.Embedding(\n",
    "            self.vocab_size, 2\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y: torch.Tensor = None):\n",
    "        logits: torch.Tensor = self.token_embedding_table(x)\n",
    "        print(logits.shape)\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # logits becomesa tensor of size (Batch size, Sequence Length (T), vocab_size)\n",
    "            B, T, C = logits.shape  # (Batch size, Sequence Length (T), vocab_size)\n",
    "            logits = logits.view(\n",
    "                B * T, C\n",
    "            )  # reshape the logits so they can be used in cross entropy loss\n",
    "            print(f\"{y=}\")\n",
    "            print(y.shape)\n",
    "            targets = y.view(B * T)\n",
    "            print(logits.shape)\n",
    "            print(targets.shape)   \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self.forward(x)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            x = torch.cat((x, idx_next), dim=1)  # (B, T+1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # this might be reflective of the encoder model but for right now i dont actually know\n",
    "        vocab_size = len(set(text))\n",
    "\n",
    "        # tokenize with byte pair encoding.\n",
    "        # This gives us a shorter token array lenght becuase we arent splitting by character\n",
    "        enc = ttk.get_encoding(\"gpt2\")\n",
    "        encoded = enc.encode(text)\n",
    "        print(f\"Text: {list(text.split())[:5]}: Length: {len(text.split())}\")\n",
    "        print(f\"Encoded: {encoded[:5]}: Length: {len(encoded)}\")\n",
    "        return enc.n_vocab, torch.tensor(encoded, dtype=torch.long)\n",
    "        # looking at the tokenized output will essentially give us a \"one to one\" translation of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, train_data, val_data, block_size, batch_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['First', 'Citizen:', 'Before', 'we', 'proceed']: Length: 155183\n",
      "Encoded: [5962, 22307, 25, 198, 8421]: Length: 255888\n",
      "vocab_size=50257\n",
      "tensor([ 5962, 22307,    25,  ...,    33,  1094, 42391])\n",
      "255888\n"
     ]
    }
   ],
   "source": [
    "vocab_size, data = process_input(\"input.txt\")\n",
    "print(f\"{vocab_size=}\")\n",
    "print(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:  255888\n",
      "Emb Size:  10\n",
      "torch.Size([5, 10, 2])\n",
      "y=tensor([[  544,   329,   470,    26,   198, 15946,  1384,   326,    11,   618],\n",
      "        [47118,  3963, 14545,  4944,    51,    25,   198,  1722,  1474,   355],\n",
      "        [ 2390,  8267,    46,    25,   198, 24749,    11,   616, 15876,     0],\n",
      "        [49654,   389,   345,  1111,    26,   198,  1870, 26246,   705, 48010],\n",
      "        [32476,   783,  3160,   287,  8838,   379,   465, 10152,    11,   198]])\n",
      "torch.Size([5, 10])\n",
      "torch.Size([50, 2])\n",
      "torch.Size([50])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 544 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_data, val_data, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m embedding_layer \u001b[38;5;241m=\u001b[39m EmbeddingLayer(vocab_size, \u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;241m10\u001b[39m, data)\n\u001b[0;32m----> 8\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m embedding_layer\u001b[38;5;241m.\u001b[39mforward(xb, yb)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m     11\u001b[0m enc \u001b[38;5;241m=\u001b[39m ttk\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[65], line 34\u001b[0m, in \u001b[0;36mEmbeddingLayer.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(targets\u001b[38;5;241m.\u001b[39mshape)   \n\u001b[0;32m---> 34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits, targets)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.12/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 544 is out of bounds."
     ]
    }
   ],
   "source": [
    "\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "# block size and batch size can change\n",
    "xb, yb = get_batch(\"train\", train_data, val_data, 10, 5)\n",
    "\n",
    "embedding_layer = EmbeddingLayer(vocab_size, len(data), 10, data)\n",
    "_, loss = embedding_layer.forward(xb, yb)\n",
    "print(loss)\n",
    "\n",
    "enc = ttk.get_encoding(\"gpt2\")\n",
    "\n",
    "decoded = enc.decode(\n",
    "    embedding_layer.generate(\n",
    "        torch.zeros(1, 1, dtype=torch.long), max_new_tokens=100\n",
    "    )[0].tolist()\n",
    ")\n",
    "# print(f\"{decoded:}\")\n",
    "# could do SGD but whatever\n",
    "optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=1e-3)\n",
    "\n",
    "# train the model\n",
    "for steps in range(100):\n",
    "    xb, yb = get_batch(\"train\", train_data, val_data, 10, 5)\n",
    "    logits, loss = embedding_layer.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
