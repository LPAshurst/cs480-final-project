{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Hyerparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken as ttk\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Modular Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward():\n",
    "        pass\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = torch.nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = torch.nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = torch.nn.ModuleList([EncoderLayer(d_model, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = torch.nn.ModuleList([DecoderLayer(d_model, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = torch.nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # this might be reflective of the encoder model but for right now i dont actually know\n",
    "        # vocab_size = len(set(text))\n",
    "\n",
    "        # tokenize with byte pair encoding.\n",
    "        # This gives us a shorter token array lenght becuase we arent splitting by character\n",
    "        enc = ttk.get_encoding(\"gpt2\")\n",
    "        encoded = enc.encode(text)\n",
    "        print(f\"Text: {list(text.split())[:5]}: Length: {len(text.split())}\")\n",
    "        print(f\"Encoded: {encoded[:5]}: Length: {len(encoded)}\")\n",
    "        return enc.n_vocab, torch.tensor(encoded, dtype=torch.long)\n",
    "        # looking at the tokenized output will essentially give us a \"one to one\" translation of the text\n",
    "\n",
    "def get_batch(split, train_data, val_data, block_size, batch_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "vocab_size, data = process_input(\"input.txt\")\n",
    "print(f\"{vocab_size=}\")\n",
    "print(data)\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_token_size, d_model, block_size, data):\n",
    "        super().__init__()\n",
    "\n",
    "        # define some important vars\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        self.d_model = d_model\n",
    "        self.input_size = input_token_size\n",
    "        \n",
    "        \n",
    "        print(\"Input Size: \", self.input_size)\n",
    "        print(\"Emb Size: \", self.d_model)\n",
    "\n",
    "        self.token_embedding_table = torch.nn.Embedding(\n",
    "            vocab_size, d_model\n",
    "        )\n",
    "        self.position_embedding_table = torch.nn.Embedding(block_size, d_model)\n",
    "\n",
    "    def forward(self, x, y: torch.Tensor = None):\n",
    "        print(\"Beginning of Forward\")\n",
    "        print(f\"{x=}\")\n",
    "        print(f\"{y=}\\n\")\n",
    "        \n",
    "        logits: torch.Tensor = self.token_embedding_table(x) \n",
    "        print(f\"{logits.shape=}\")\n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # logits becomes a tensor of size (Batch size, Sequence Length (T), vocab_size)\n",
    "            B, T, C = logits.shape  # (Batch size, Sequence Length (T), vocab_size)\n",
    "            logits = logits.view(\n",
    "                B * T, C\n",
    "            )  # reshape the logits so they can be used in cross entropy loss\n",
    "            print(f\"{y=}\")\n",
    "            print(f\"{y.shape=}\")\n",
    "            print(f\"{type(y)=}\")\n",
    "            targets = y.view(B * T)\n",
    "            print(f\"{logits.shape=} {logits=}\")\n",
    "            print(f\"{targets.shape=} {targets=}\")   \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, _ = self.forward(x)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            x = torch.cat((x, idx_next), dim=1)  # (B, T+1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:  255888\n",
      "Emb Size:  20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "# block size and batch size can change\n",
    "block_size = 10\n",
    "batch_size = 5\n",
    "xb, yb = get_batch(\"train\", train_data, val_data, block_size, batch_size)\n",
    "\n",
    "d_model = 20\n",
    "embedding_layer = EmbeddingLayer(vocab_size, len(data), d_model, block_size, data)\n",
    "# embedding_layer.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of Forward\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[  198,   198,    49, 30194,    25,   198, 22788,  6219, 11738,   565],\n",
      "        [  286,  1971,   338,    11,   198,  2504,  1560,  2042, 29770,   654],\n",
      "        [ 3428,  2951,   198, 23792,   465,  1490,   496,    11,   290,   326],\n",
      "        [16599,     0,   198,    49,  1191,   290,   360, 49590,    11,   345],\n",
      "        [  465,  3956,    11,   290,   314,  1842,   683,   880,    13,   198]])\n",
      "y=tensor([[  198,    49, 30194,    25,   198, 22788,  6219, 11738,   565,   733],\n",
      "        [ 1971,   338,    11,   198,  2504,  1560,  2042, 29770,   654,    13],\n",
      "        [ 2951,   198, 23792,   465,  1490,   496,    11,   290,   326,   477],\n",
      "        [    0,   198,    49,  1191,   290,   360, 49590,    11,   345,   547],\n",
      "        [ 3956,    11,   290,   314,  1842,   683,   880,    13,   198,  1532]])\n",
      "\n",
      "logits.shape=torch.Size([5, 10, 20])\n",
      "y=tensor([[  198,    49, 30194,    25,   198, 22788,  6219, 11738,   565,   733],\n",
      "        [ 1971,   338,    11,   198,  2504,  1560,  2042, 29770,   654,    13],\n",
      "        [ 2951,   198, 23792,   465,  1490,   496,    11,   290,   326,   477],\n",
      "        [    0,   198,    49,  1191,   290,   360, 49590,    11,   345,   547],\n",
      "        [ 3956,    11,   290,   314,  1842,   683,   880,    13,   198,  1532]])\n",
      "y.shape=torch.Size([5, 10])\n",
      "type(y)=<class 'torch.Tensor'>\n",
      "logits.shape=torch.Size([50, 20]) logits=tensor([[ 3.5613e-02,  4.7594e-01, -3.2673e-01, -6.1577e-01, -6.3488e-01,\n",
      "         -1.3419e+00, -1.9459e-01, -5.9932e-01, -1.4445e+00,  4.3305e-01,\n",
      "         -2.3531e+00,  5.6935e-01, -9.5410e-01,  1.9816e+00,  1.3189e+00,\n",
      "          3.4421e-01, -1.4432e+00, -8.7938e-01, -7.8211e-01,  1.2196e+00],\n",
      "        [ 3.5613e-02,  4.7594e-01, -3.2673e-01, -6.1577e-01, -6.3488e-01,\n",
      "         -1.3419e+00, -1.9459e-01, -5.9932e-01, -1.4445e+00,  4.3305e-01,\n",
      "         -2.3531e+00,  5.6935e-01, -9.5410e-01,  1.9816e+00,  1.3189e+00,\n",
      "          3.4421e-01, -1.4432e+00, -8.7938e-01, -7.8211e-01,  1.2196e+00],\n",
      "        [ 5.3869e-01, -7.0697e-01, -1.8313e+00, -3.7388e-02, -5.4787e-01,\n",
      "         -1.9607e+00, -3.2925e-02,  1.7421e+00, -1.5711e+00,  2.2208e-01,\n",
      "          1.1168e+00,  1.0792e+00, -1.3860e+00, -1.1101e-01, -1.4399e+00,\n",
      "         -1.3932e+00, -5.1932e-01, -4.8859e-01, -1.4399e+00, -2.3114e+00],\n",
      "        [ 9.0742e-01,  5.9868e-01,  1.3469e-01,  2.5983e-01, -4.8884e-01,\n",
      "         -6.1681e-01,  1.5783e+00, -4.2075e-01,  2.4667e+00, -1.9699e+00,\n",
      "          9.4016e-01,  2.9854e-02, -5.3700e-01,  8.7143e-01,  1.2877e-01,\n",
      "         -4.8954e-01,  1.8194e-01, -1.0507e+00,  2.8505e+00, -1.2529e+00],\n",
      "        [-6.2456e-01, -8.6179e-01,  6.2017e-01, -3.0090e-01,  5.0517e-01,\n",
      "         -1.3446e+00, -2.7926e-01,  1.7092e-01,  1.6831e+00, -4.7541e-01,\n",
      "         -1.1004e+00, -3.6776e-01, -8.0372e-01, -2.8934e-02, -5.1516e-01,\n",
      "          7.1387e-01,  3.7589e-01, -7.6028e-01, -3.1591e-01, -5.4644e-01],\n",
      "        [ 3.5613e-02,  4.7594e-01, -3.2673e-01, -6.1577e-01, -6.3488e-01,\n",
      "         -1.3419e+00, -1.9459e-01, -5.9932e-01, -1.4445e+00,  4.3305e-01,\n",
      "         -2.3531e+00,  5.6935e-01, -9.5410e-01,  1.9816e+00,  1.3189e+00,\n",
      "          3.4421e-01, -1.4432e+00, -8.7938e-01, -7.8211e-01,  1.2196e+00],\n",
      "        [-6.7656e-02,  2.4243e-02,  7.8356e-01, -1.9203e+00,  9.6770e-01,\n",
      "          6.7165e-01,  5.4271e-01,  1.1265e+00,  5.1263e-01, -4.8501e-01,\n",
      "          4.3229e-01, -4.5526e-01,  4.4399e-01, -7.1485e-01,  6.0586e-02,\n",
      "         -1.6417e+00,  8.5875e-01,  5.7677e-01,  5.6649e-01,  6.6856e-01],\n",
      "        [ 6.5338e-01, -5.5878e-01,  1.0883e+00, -2.2593e-01,  5.5804e-01,\n",
      "          6.5708e-01,  1.9975e+00,  2.2251e-01, -2.1772e+00, -2.2265e+00,\n",
      "          6.0551e-03,  1.7992e-02, -1.1746e+00,  3.7982e-01,  9.9882e-01,\n",
      "          1.3926e+00,  5.4382e-01, -3.1656e-01, -5.9545e-01, -4.4543e-01],\n",
      "        [ 5.6854e-01, -1.3417e+00, -1.6758e+00,  1.9647e-01, -3.3657e-01,\n",
      "         -1.2576e+00,  2.3891e+00,  1.6864e-01,  9.2451e-01, -5.4850e-01,\n",
      "          9.2727e-01, -5.4739e-03,  1.9547e+00, -1.3206e+00, -2.5637e-01,\n",
      "         -5.0968e-01, -7.1880e-01,  1.0082e+00, -2.6982e+00, -5.9279e-01],\n",
      "        [ 2.7807e-01,  1.2309e+00,  1.1204e+00, -7.0096e-02,  1.3705e-01,\n",
      "          7.1318e-02,  1.1290e+00,  1.9110e+00,  3.5433e-01,  6.2590e-02,\n",
      "          5.0197e-02, -6.2225e-01, -2.9063e-01, -3.9804e-01, -4.8233e-01,\n",
      "         -1.0486e+00,  5.1275e-01,  1.3442e+00, -9.1400e-01,  3.7556e-01],\n",
      "        [-3.5096e-01,  1.1398e+00,  3.7885e-01, -1.7155e+00,  1.5812e-01,\n",
      "         -6.8077e-01,  1.3761e+00, -1.0043e+00, -4.2510e-01, -5.3119e-01,\n",
      "         -6.7824e-03, -1.8431e+00, -7.5844e-01, -1.0529e+00, -2.7811e-01,\n",
      "          1.7514e-01,  1.4931e-01, -4.3812e-01, -2.1604e+00,  1.2528e+00],\n",
      "        [-7.4181e-01, -2.6111e-01, -1.8009e+00, -1.2862e-01,  3.9204e-01,\n",
      "         -5.4565e-01,  6.9623e-01,  6.7795e-01,  7.8488e-02,  3.7229e-01,\n",
      "         -5.9797e-01,  7.5612e-01, -8.6279e-01,  7.4670e-01, -5.0048e-01,\n",
      "         -1.1092e-02, -1.5963e+00,  1.3357e+00, -1.1596e+00,  2.2021e+00],\n",
      "        [ 3.8675e-01, -1.3219e+00, -7.5993e-01, -4.6477e-01,  1.1199e+00,\n",
      "         -8.6252e-01,  8.6510e-01, -2.1710e-01, -4.7214e-02,  3.7556e-01,\n",
      "          1.8407e-01, -1.6369e+00, -1.1425e+00,  2.2749e+00,  1.8103e-02,\n",
      "         -7.3747e-03, -5.0342e-01, -2.7930e-01,  7.7284e-01, -1.3819e+00],\n",
      "        [ 5.3969e-01, -1.3920e+00,  5.7145e-01,  6.4937e-01,  1.1944e-01,\n",
      "         -4.0919e-01,  2.3667e-01,  1.1520e+00, -7.6249e-02, -9.2236e-01,\n",
      "          9.2423e-01,  2.2067e+00,  1.1415e+00,  7.8624e-01,  1.0808e+00,\n",
      "         -2.7518e-02, -7.0034e-01,  1.0059e+00,  7.3207e-02, -1.0663e+00],\n",
      "        [ 3.5613e-02,  4.7594e-01, -3.2673e-01, -6.1577e-01, -6.3488e-01,\n",
      "         -1.3419e+00, -1.9459e-01, -5.9932e-01, -1.4445e+00,  4.3305e-01,\n",
      "         -2.3531e+00,  5.6935e-01, -9.5410e-01,  1.9816e+00,  1.3189e+00,\n",
      "          3.4421e-01, -1.4432e+00, -8.7938e-01, -7.8211e-01,  1.2196e+00],\n",
      "        [-1.3716e+00,  3.6488e-01,  1.7560e+00,  6.9476e-01, -1.2089e+00,\n",
      "         -1.1372e+00, -5.3142e-01,  1.5203e+00, -8.1494e-01,  6.3785e-01,\n",
      "         -4.8102e-01,  2.7132e-01,  6.5781e-01,  1.1903e+00,  1.3367e+00,\n",
      "         -2.5967e-01, -2.1119e+00,  2.6830e-01,  1.4357e+00, -3.4534e-01],\n",
      "        [ 2.0513e+00,  1.3867e+00,  3.2301e-01,  2.6956e+00,  9.8431e-01,\n",
      "          5.5310e-01,  1.3588e-01, -2.0487e+00, -2.4944e+00,  2.6581e-01,\n",
      "         -1.6047e+00,  2.9101e-01, -1.3573e-01, -6.2062e-01, -7.7525e-01,\n",
      "         -1.5869e+00, -1.4327e+00,  6.5311e-01,  7.3719e-02, -4.3835e-02],\n",
      "        [-3.8258e-01,  1.1103e+00, -7.9089e-01,  4.0681e-01,  2.3886e-01,\n",
      "          1.4804e+00,  1.3068e+00, -7.7820e-02, -1.8514e+00,  6.8608e-01,\n",
      "         -4.0802e+00,  2.8248e-01,  1.8652e-01,  8.0585e-01,  8.1476e-01,\n",
      "          1.2064e+00,  6.6495e-01, -4.5840e-02,  1.3823e-01,  4.5104e-01],\n",
      "        [ 3.7028e-01,  8.7099e-01,  6.7997e-01, -1.0892e+00, -1.2038e+00,\n",
      "         -1.9124e+00,  4.3411e+00,  1.8373e+00,  5.1853e-01, -6.4477e-01,\n",
      "         -3.1274e-01,  8.4331e-02, -7.5195e-01, -2.7997e-01, -2.2676e-01,\n",
      "          8.1390e-01, -3.5923e-01,  2.0602e-01,  1.4408e-01, -8.3176e-01],\n",
      "        [-5.1621e-01,  8.6595e-01,  7.1544e-03,  7.1488e-01,  1.0773e+00,\n",
      "         -8.0130e-01, -1.4082e+00,  8.9425e-01, -1.1745e+00, -2.1727e-01,\n",
      "         -1.1197e+00,  5.8345e-01, -7.3222e-01,  4.6792e-01,  2.0688e+00,\n",
      "          1.4139e-01,  2.7346e-01, -2.0280e-01, -1.3234e+00, -3.4196e-01],\n",
      "        [-5.4566e-01,  2.0146e-01, -8.4970e-02, -1.0034e+00,  7.7098e-01,\n",
      "         -7.4929e-02,  1.6340e+00, -5.7102e-01,  2.7526e-01,  9.9149e-02,\n",
      "          1.5206e-01,  1.3448e-01,  5.2485e-01, -1.2183e-02,  5.6930e-01,\n",
      "          6.5403e-01,  4.3934e-01,  1.1488e-01,  3.8345e-01,  1.1892e+00],\n",
      "        [ 7.5018e-02,  6.7604e-01,  7.8609e-01, -1.7778e-01, -8.2515e-01,\n",
      "         -2.6514e-01,  6.4643e-01, -5.0063e-01,  3.0864e-01,  2.6307e+00,\n",
      "          8.2681e-01, -2.2164e-01, -2.1556e-01,  1.0918e+00,  1.3377e+00,\n",
      "          1.2751e+00,  1.7919e+00, -7.6213e-02, -5.2039e-02, -9.7429e-01],\n",
      "        [ 3.5613e-02,  4.7594e-01, -3.2673e-01, -6.1577e-01, -6.3488e-01,\n",
      "         -1.3419e+00, -1.9459e-01, -5.9932e-01, -1.4445e+00,  4.3305e-01,\n",
      "         -2.3531e+00,  5.6935e-01, -9.5410e-01,  1.9816e+00,  1.3189e+00,\n",
      "          3.4421e-01, -1.4432e+00, -8.7938e-01, -7.8211e-01,  1.2196e+00],\n",
      "        [ 4.7540e-01,  3.8059e-01,  1.5611e-01,  1.2983e+00,  1.1854e+00,\n",
      "         -6.0004e-01, -6.8123e-01,  2.3274e+00, -1.0220e+00, -1.0196e+00,\n",
      "         -1.4924e+00, -3.5750e-01, -8.5999e-01, -1.6865e-01, -2.9131e-01,\n",
      "         -1.4197e+00, -8.7792e-01,  2.3781e+00,  4.1724e-01, -1.8264e+00],\n",
      "        [-1.3906e+00, -2.2184e+00,  1.0362e+00,  1.2941e-01,  1.9510e-01,\n",
      "          7.6310e-01, -9.4312e-01,  1.5151e+00,  2.0521e+00, -3.4601e-01,\n",
      "          8.4671e-01, -1.7346e+00, -1.1652e+00, -9.3967e-02, -1.0639e+00,\n",
      "          1.2806e+00, -4.0271e-01,  4.1638e-01, -2.4033e+00,  3.4303e-01],\n",
      "        [ 1.9594e-01,  2.6512e+00, -8.8296e-01,  1.1396e+00, -1.4415e+00,\n",
      "         -4.5841e-01, -9.2733e-01, -8.7623e-01, -6.8536e-02,  4.1422e-02,\n",
      "         -4.0817e-01, -1.1732e-01, -1.8535e+00,  2.8207e-01, -6.7739e-01,\n",
      "         -1.2857e+00,  1.3933e+00,  2.6715e+00, -2.0035e-01,  1.1813e+00],\n",
      "        [-1.9315e+00, -9.2518e-01, -2.3806e-01,  9.7828e-02,  6.1753e-01,\n",
      "         -7.0794e-01, -2.6363e-01, -1.1868e+00,  5.1497e-01, -2.0399e-01,\n",
      "         -1.5125e-01, -1.1989e+00,  4.4448e-01,  1.0107e+00,  1.2865e+00,\n",
      "         -2.3992e-01,  1.0882e+00, -1.0678e+00,  6.6186e-01,  1.2540e-01],\n",
      "        [ 5.3969e-01, -1.3920e+00,  5.7145e-01,  6.4937e-01,  1.1944e-01,\n",
      "         -4.0919e-01,  2.3667e-01,  1.1520e+00, -7.6249e-02, -9.2236e-01,\n",
      "          9.2423e-01,  2.2067e+00,  1.1415e+00,  7.8624e-01,  1.0808e+00,\n",
      "         -2.7518e-02, -7.0034e-01,  1.0059e+00,  7.3207e-02, -1.0663e+00],\n",
      "        [ 6.1514e-01,  1.2731e+00,  9.4885e-02,  5.9164e-01,  3.2683e-01,\n",
      "         -9.2493e-01, -1.8359e+00,  7.9007e-01, -4.9027e-01,  1.8772e+00,\n",
      "         -5.5070e-01, -8.6529e-01,  1.2001e+00, -4.5562e-01,  1.9798e+00,\n",
      "         -6.8396e-01,  7.1197e-01, -2.0527e-01, -3.3841e-01, -1.3186e+00],\n",
      "        [ 5.7603e-01,  2.3249e-01,  3.4352e-02,  2.3989e-01, -5.3197e-01,\n",
      "          3.2806e-01,  2.4012e-01,  1.0456e-01, -1.2186e+00,  1.6475e+00,\n",
      "          3.3981e-01, -2.4271e+00, -1.0181e+00, -5.3921e-01,  7.3310e-01,\n",
      "         -4.7472e-01, -1.4441e+00, -5.7331e-01,  3.1560e-01,  6.0676e-01],\n",
      "        [ 1.4642e+00, -1.2541e-01, -7.8547e-01, -2.6062e-01,  9.2276e-01,\n",
      "         -5.0660e-01, -5.7499e-01,  1.1158e+00, -3.6762e-01,  8.3074e-01,\n",
      "          1.0621e+00, -1.3116e-01, -8.8403e-01, -1.0715e+00,  1.0088e+00,\n",
      "          1.6855e-01, -2.3238e+00,  1.7744e+00,  3.2479e-01, -1.1502e+00],\n",
      "        [-4.1684e-01, -1.1383e+00, -1.9130e+00, -1.6071e+00, -1.5952e+00,\n",
      "         -5.4333e-01, -1.5995e-01, -8.6242e-01,  1.8465e-01,  1.3380e+00,\n",
      "         -7.1152e-01, -8.2570e-01,  2.3644e+00,  2.3735e-01,  1.3706e+00,\n",
      "         -3.5283e-01,  2.1401e+00,  1.9150e-01,  1.5349e+00,  1.1756e+00],\n",
      "        [ 3.5613e-02,  4.7594e-01, -3.2673e-01, -6.1577e-01, -6.3488e-01,\n",
      "         -1.3419e+00, -1.9459e-01, -5.9932e-01, -1.4445e+00,  4.3305e-01,\n",
      "         -2.3531e+00,  5.6935e-01, -9.5410e-01,  1.9816e+00,  1.3189e+00,\n",
      "          3.4421e-01, -1.4432e+00, -8.7938e-01, -7.8211e-01,  1.2196e+00],\n",
      "        [ 5.3869e-01, -7.0697e-01, -1.8313e+00, -3.7388e-02, -5.4787e-01,\n",
      "         -1.9607e+00, -3.2925e-02,  1.7421e+00, -1.5711e+00,  2.2208e-01,\n",
      "          1.1168e+00,  1.0792e+00, -1.3860e+00, -1.1101e-01, -1.4399e+00,\n",
      "         -1.3932e+00, -5.1932e-01, -4.8859e-01, -1.4399e+00, -2.3114e+00],\n",
      "        [-1.1155e-01, -4.9160e-01, -1.8528e+00,  8.6974e-01,  1.1422e+00,\n",
      "         -3.6294e-01, -2.1331e-01,  7.5706e-01, -8.5940e-02, -2.0481e-01,\n",
      "          1.9130e-01, -2.6211e+00,  9.5505e-02, -9.2275e-01,  5.7000e-01,\n",
      "          2.1706e-01, -1.9390e-01, -5.5216e-01, -3.2112e-01,  2.0013e+00],\n",
      "        [ 6.1514e-01,  1.2731e+00,  9.4885e-02,  5.9164e-01,  3.2683e-01,\n",
      "         -9.2493e-01, -1.8359e+00,  7.9007e-01, -4.9027e-01,  1.8772e+00,\n",
      "         -5.5070e-01, -8.6529e-01,  1.2001e+00, -4.5562e-01,  1.9798e+00,\n",
      "         -6.8396e-01,  7.1197e-01, -2.0527e-01, -3.3841e-01, -1.3186e+00],\n",
      "        [ 1.1579e+00, -1.6576e+00, -2.2647e+00, -7.9033e-02,  9.7590e-01,\n",
      "         -9.7199e-01,  2.4925e-01,  7.5028e-01,  1.1284e+00,  1.7986e+00,\n",
      "          1.2399e+00, -8.5414e-02,  3.3994e-01, -1.8058e+00,  2.5417e+00,\n",
      "         -7.3813e-01, -4.5410e-01,  1.5993e-01, -8.6723e-01,  7.9535e-01],\n",
      "        [ 3.1221e-01, -1.7009e+00, -1.7797e+00, -5.2888e-01,  5.6348e-01,\n",
      "         -6.9212e-01, -1.0991e+00, -9.3878e-01,  4.6412e-01,  1.2886e+00,\n",
      "          7.1781e-01,  1.3465e+00, -1.4031e+00,  6.6594e-01, -1.1739e+00,\n",
      "         -5.5521e-01,  6.6139e-01,  4.1281e-01,  7.4881e-01, -2.2685e+00],\n",
      "        [ 5.3969e-01, -1.3920e+00,  5.7145e-01,  6.4937e-01,  1.1944e-01,\n",
      "         -4.0919e-01,  2.3667e-01,  1.1520e+00, -7.6249e-02, -9.2236e-01,\n",
      "          9.2423e-01,  2.2067e+00,  1.1415e+00,  7.8624e-01,  1.0808e+00,\n",
      "         -2.7518e-02, -7.0034e-01,  1.0059e+00,  7.3207e-02, -1.0663e+00],\n",
      "        [-1.2531e-01,  2.3403e+00,  7.6414e-01, -1.0759e-01,  2.7227e-01,\n",
      "          1.4179e+00, -6.5055e-01, -1.1486e-01,  4.0672e-01,  1.3272e+00,\n",
      "         -3.2690e-01,  2.0133e-01, -2.9136e+00, -4.9227e-03, -4.0164e-01,\n",
      "         -1.2741e+00, -5.5125e-01, -2.4803e-01, -6.8801e-01, -3.3724e-01],\n",
      "        [-1.3906e+00, -2.2184e+00,  1.0362e+00,  1.2941e-01,  1.9510e-01,\n",
      "          7.6310e-01, -9.4312e-01,  1.5151e+00,  2.0521e+00, -3.4601e-01,\n",
      "          8.4671e-01, -1.7346e+00, -1.1652e+00, -9.3967e-02, -1.0639e+00,\n",
      "          1.2806e+00, -4.0271e-01,  4.1638e-01, -2.4033e+00,  3.4303e-01],\n",
      "        [-3.1283e-01, -2.7591e-01, -4.1657e-01,  1.5288e+00, -5.9442e-01,\n",
      "          5.4931e-01, -1.0805e+00, -6.3005e-01,  9.8644e-01,  1.9424e-01,\n",
      "         -1.5497e-01, -7.9433e-01,  1.1553e-01,  1.6034e-01, -9.6996e-01,\n",
      "         -1.2429e+00, -6.1003e-01, -8.8905e-01,  6.3436e-01, -2.3295e+00],\n",
      "        [ 5.3969e-01, -1.3920e+00,  5.7145e-01,  6.4937e-01,  1.1944e-01,\n",
      "         -4.0919e-01,  2.3667e-01,  1.1520e+00, -7.6249e-02, -9.2236e-01,\n",
      "          9.2423e-01,  2.2067e+00,  1.1415e+00,  7.8624e-01,  1.0808e+00,\n",
      "         -2.7518e-02, -7.0034e-01,  1.0059e+00,  7.3207e-02, -1.0663e+00],\n",
      "        [ 6.1514e-01,  1.2731e+00,  9.4885e-02,  5.9164e-01,  3.2683e-01,\n",
      "         -9.2493e-01, -1.8359e+00,  7.9007e-01, -4.9027e-01,  1.8772e+00,\n",
      "         -5.5070e-01, -8.6529e-01,  1.2001e+00, -4.5562e-01,  1.9798e+00,\n",
      "         -6.8396e-01,  7.1197e-01, -2.0527e-01, -3.3841e-01, -1.3186e+00],\n",
      "        [ 7.7360e-01,  3.1285e-01, -2.4062e-01,  1.0526e-01,  1.7913e-01,\n",
      "         -6.8571e-01, -3.7520e-01,  4.5452e-01,  1.1707e+00, -6.1461e-02,\n",
      "          2.3803e-01,  4.3330e-01,  7.2577e-01,  1.2659e+00,  2.5213e-01,\n",
      "          4.3385e-01, -3.8414e-01, -3.7655e-01, -1.0653e+00, -9.9874e-01],\n",
      "        [ 1.2613e-02,  1.7245e+00,  1.4180e-02, -3.2367e-01, -1.5088e-01,\n",
      "          6.9971e-01, -1.3670e+00,  3.5094e-02,  9.5513e-01, -3.2095e-01,\n",
      "          2.8259e+00,  9.8823e-01, -1.2835e-01,  7.2556e-01,  2.5907e-01,\n",
      "          2.0475e+00,  9.4575e-01, -2.5785e-01, -1.4905e+00,  1.5189e+00],\n",
      "        [-8.2888e-02, -1.4786e+00, -1.4788e+00,  1.7821e+00,  6.6776e-01,\n",
      "         -1.0662e+00,  1.2549e+00,  7.8455e-01,  1.5710e+00, -1.1619e+00,\n",
      "         -1.0959e+00,  1.0328e+00,  6.2530e-01,  2.0492e+00, -1.6360e+00,\n",
      "          2.9151e-01,  2.2393e-03, -6.9047e-01, -9.2785e-01, -3.7611e-01],\n",
      "        [-2.9509e-01,  7.9788e-01,  2.2287e-01,  1.0322e+00,  9.8824e-01,\n",
      "          2.5233e+00,  1.4675e+00,  4.6468e-01,  2.8420e+00, -5.3979e-01,\n",
      "          4.1201e-01, -8.0600e-03,  9.0190e-01,  3.9234e-01, -8.8438e-01,\n",
      "         -6.6229e-01, -7.1534e-01, -1.2255e+00,  1.7564e+00, -8.6698e-01],\n",
      "        [ 1.5405e-01, -1.2173e+00,  2.7176e-01, -1.3498e-01,  7.7476e-01,\n",
      "         -1.8321e-01, -4.1748e-01,  7.5297e-01, -2.9517e-01, -3.3322e-02,\n",
      "         -3.8054e+00,  1.3264e+00, -1.3464e+00, -5.1759e-02,  7.7782e-01,\n",
      "         -9.3775e-01,  5.6547e-01, -7.0566e-01, -3.1284e-01,  1.8106e-01],\n",
      "        [ 3.5613e-02,  4.7594e-01, -3.2673e-01, -6.1577e-01, -6.3488e-01,\n",
      "         -1.3419e+00, -1.9459e-01, -5.9932e-01, -1.4445e+00,  4.3305e-01,\n",
      "         -2.3531e+00,  5.6935e-01, -9.5410e-01,  1.9816e+00,  1.3189e+00,\n",
      "          3.4421e-01, -1.4432e+00, -8.7938e-01, -7.8211e-01,  1.2196e+00]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "targets.shape=torch.Size([50]) targets=tensor([  198,    49, 30194,    25,   198, 22788,  6219, 11738,   565,   733,\n",
      "         1971,   338,    11,   198,  2504,  1560,  2042, 29770,   654,    13,\n",
      "         2951,   198, 23792,   465,  1490,   496,    11,   290,   326,   477,\n",
      "            0,   198,    49,  1191,   290,   360, 49590,    11,   345,   547,\n",
      "         3956,    11,   290,   314,  1842,   683,   880,    13,   198,  1532])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 198 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m \u001b[43membedding_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m      3\u001b[0m exit()\n",
      "Cell \u001b[0;32mIn[26], line 41\u001b[0m, in \u001b[0;36mEmbeddingLayer.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargets\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargets\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)   \n\u001b[0;32m---> 41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 198 is out of bounds."
     ]
    }
   ],
   "source": [
    "_, loss = embedding_layer.forward(xb, yb)\n",
    "print(loss)\n",
    "exit()\n",
    "\n",
    "enc = ttk.get_encoding(\"gpt2\")\n",
    "\n",
    "decoded = enc.decode(\n",
    "    embedding_layer.generate(\n",
    "        torch.zeros(1, 1, dtype=torch.long), max_new_tokens=100\n",
    "    )[0].tolist()\n",
    ")\n",
    "# print(f\"{decoded:}\")\n",
    "# could do SGD but whatever\n",
    "optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=1e-3)\n",
    "\n",
    "# train the model\n",
    "for steps in range(100):\n",
    "    xb, yb = get_batch(\"train\", train_data, val_data, 10, 5)\n",
    "    logits, loss = embedding_layer.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
