{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Hyerparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "context_length = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "torch.set_default_device(\"cuda\") if torch.cuda.is_available() else torch.set_default_device(\"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # Use GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # Use CPU\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Modular Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward():\n",
    "        pass\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = torch.nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = torch.nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = torch.nn.ModuleList([EncoderLayer(d_model, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = torch.nn.ModuleList([DecoderLayer(d_model, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = torch.nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m         center \u001b[38;5;241m=\u001b[39m data[i]\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m context, center\n\u001b[0;32m---> 24\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mprocess_input\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m tokenized_ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(tokenize_batch, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m flat_encoded \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenized_ds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m example]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# im also thinking that maybe the length of the encoded list is good? maybe?\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(flat_encoded, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/datasets/dataset_dict.py:82\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     85\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     86\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_ids'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_294660/461433027.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(\"data.pt\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "def process_input():\n",
    "\n",
    "    ds = load_dataset(\"upstage/Pretraining_Dataset\", cache_dir=\"data\") \n",
    "\n",
    "    def tokenize_batch(batch):\n",
    "        return tokenizer(batch[\"text\"], padding=False, truncation=False)\n",
    "\n",
    "    tokenized_ds = ds.map(tokenize_batch, batched=True)\n",
    "    flat_encoded = [token for example in tokenized_ds[\"input_ids\"] for token in example]\n",
    "    # im also thinking that maybe the length of the encoded list is good? maybe?\n",
    "    return torch.tensor(flat_encoded, dtype=torch.long)\n",
    "    # looking at the tokenized output will essentially give us a \"one to one\" translation of the text\n",
    "\n",
    "def build_cbow_pairs(data, context_size=2):\n",
    "    for i in range(context_size, len(data) - context_size):\n",
    "        left = data[i - context_size:i]\n",
    "        right = data[i + 1:i + context_size + 1]\n",
    "        context = torch.cat((left, right))\n",
    "        center = data[i]\n",
    "        yield context, center\n",
    "\n",
    "data = process_input()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_token_size, d_model, context_length, data):\n",
    "        super().__init__()\n",
    "\n",
    "        # define some important vars\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "        self.d_model = d_model\n",
    "        self.input_size = input_token_size\n",
    "\n",
    "        self.token_embedding_table = torch.nn.Embedding(\n",
    "            vocab_size + 1, d_model, \n",
    "        )\n",
    "        self.linear_one = torch.nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds: torch.Tensor = self.token_embedding_table(x)\n",
    "        input_embeds = embeds.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        out: torch.Tensor = self.linear_one(input_embeds)\n",
    "        return out.squeeze(0)\n",
    "\n",
    "    # def generate(self, x, max_new_tokens):\n",
    "    #     for _ in range(max_new_tokens):\n",
    "    #         # get the predictions\n",
    "    #         logits, _ = self.forward(x)\n",
    "    #         # focus only on the last time step\n",
    "    #         logits = logits[:, -1, :]  # becomes (B, C)\n",
    "    #         # apply softmax to get probabilities\n",
    "    #         probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "    #         # sample from the distribution\n",
    "    #         idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "    #         # append sampled index to the running sequence\n",
    "    #         x = torch.cat((x, idx_next), dim=1)  # (B, T+1)\n",
    "    #     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_model = 8\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_layer = EmbeddingLayer(vocab_size, vocab_size, d_model, context_length, data)\n",
    "\n",
    "load_state = 0 # 0 = TRAIN MODEL. 1 = LOAD MODEL\n",
    "if load_state:\n",
    "    embedding_layer.load_state_dict(torch.load('word_embedding.pth'))\n",
    "    embedding_layer.eval()\n",
    "# embedding_layer.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = build_cbow_pairs(data, context_length)\n",
    "sum(1 for _ in pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000, Avg loss (last 1000): 6.9519\n",
      "Step 2000, Avg loss (last 1000): 6.9052\n",
      "Step 3000, Avg loss (last 1000): 7.2924\n",
      "Step 4000, Avg loss (last 1000): 7.3545\n",
      "Step 5000, Avg loss (last 1000): 7.3199\n",
      "Step 6000, Avg loss (last 1000): 7.0895\n",
      "Step 7000, Avg loss (last 1000): 7.3692\n",
      "Step 8000, Avg loss (last 1000): 7.4065\n",
      "Step 9000, Avg loss (last 1000): 7.3667\n",
      "Step 10000, Avg loss (last 1000): 7.6650\n",
      "Step 11000, Avg loss (last 1000): 7.7267\n",
      "Step 12000, Avg loss (last 1000): 8.5399\n",
      "Step 13000, Avg loss (last 1000): 10.4294\n",
      "Step 14000, Avg loss (last 1000): 10.7918\n",
      "Step 15000, Avg loss (last 1000): 10.4098\n",
      "Step 16000, Avg loss (last 1000): 10.0655\n",
      "Step 17000, Avg loss (last 1000): 10.2821\n",
      "Step 18000, Avg loss (last 1000): 10.4888\n",
      "Step 19000, Avg loss (last 1000): 10.4932\n",
      "Step 20000, Avg loss (last 1000): 10.1903\n",
      "Step 21000, Avg loss (last 1000): 9.0885\n",
      "Step 22000, Avg loss (last 1000): 8.5195\n",
      "Step 23000, Avg loss (last 1000): 8.4147\n",
      "Step 24000, Avg loss (last 1000): 7.3347\n",
      "Step 25000, Avg loss (last 1000): 8.5179\n",
      "Step 26000, Avg loss (last 1000): 10.0029\n",
      "Step 27000, Avg loss (last 1000): 10.1494\n",
      "Step 28000, Avg loss (last 1000): 10.7847\n",
      "Step 29000, Avg loss (last 1000): 11.3176\n",
      "Step 30000, Avg loss (last 1000): 11.3499\n",
      "Step 31000, Avg loss (last 1000): 10.5580\n",
      "Step 32000, Avg loss (last 1000): 10.9652\n",
      "Step 33000, Avg loss (last 1000): 10.6272\n",
      "Step 34000, Avg loss (last 1000): 11.1645\n",
      "Step 35000, Avg loss (last 1000): 9.9456\n",
      "Step 36000, Avg loss (last 1000): 11.3606\n",
      "Step 37000, Avg loss (last 1000): 10.0037\n",
      "Step 38000, Avg loss (last 1000): 10.4410\n",
      "Step 39000, Avg loss (last 1000): 10.6373\n",
      "Step 40000, Avg loss (last 1000): 10.6752\n",
      "Step 41000, Avg loss (last 1000): 10.5633\n",
      "Step 42000, Avg loss (last 1000): 10.2191\n",
      "Step 43000, Avg loss (last 1000): 10.7237\n",
      "Step 44000, Avg loss (last 1000): 11.6243\n",
      "Step 45000, Avg loss (last 1000): 10.7099\n",
      "Step 46000, Avg loss (last 1000): 9.9384\n",
      "Step 47000, Avg loss (last 1000): 10.2152\n",
      "Step 48000, Avg loss (last 1000): 10.3733\n",
      "Step 49000, Avg loss (last 1000): 10.8136\n",
      "Step 50000, Avg loss (last 1000): 10.4073\n",
      "Step 51000, Avg loss (last 1000): 10.4463\n",
      "Step 52000, Avg loss (last 1000): 10.6669\n",
      "Step 53000, Avg loss (last 1000): 10.4186\n",
      "Step 54000, Avg loss (last 1000): 10.7582\n",
      "Step 55000, Avg loss (last 1000): 10.5392\n",
      "Step 56000, Avg loss (last 1000): 10.9073\n",
      "Step 57000, Avg loss (last 1000): 10.9779\n",
      "Step 58000, Avg loss (last 1000): 10.1474\n",
      "Step 59000, Avg loss (last 1000): 10.1121\n",
      "Step 60000, Avg loss (last 1000): 10.2585\n",
      "Step 61000, Avg loss (last 1000): 10.0136\n",
      "Step 62000, Avg loss (last 1000): 10.6209\n",
      "Step 63000, Avg loss (last 1000): 11.1354\n",
      "Step 64000, Avg loss (last 1000): 10.7943\n",
      "Step 65000, Avg loss (last 1000): 10.6518\n",
      "Step 66000, Avg loss (last 1000): 10.2922\n",
      "Step 67000, Avg loss (last 1000): 10.8515\n",
      "Step 68000, Avg loss (last 1000): 10.2846\n",
      "Step 69000, Avg loss (last 1000): 10.5936\n",
      "Step 70000, Avg loss (last 1000): 10.1095\n",
      "Step 71000, Avg loss (last 1000): 10.3429\n",
      "Step 72000, Avg loss (last 1000): 10.9233\n",
      "Step 73000, Avg loss (last 1000): 10.8390\n",
      "Step 74000, Avg loss (last 1000): 10.8147\n",
      "Step 75000, Avg loss (last 1000): 12.1879\n",
      "Step 76000, Avg loss (last 1000): 10.6159\n",
      "Step 77000, Avg loss (last 1000): 10.6350\n",
      "Step 78000, Avg loss (last 1000): 10.1992\n",
      "Step 79000, Avg loss (last 1000): 10.6482\n",
      "Step 80000, Avg loss (last 1000): 10.4164\n",
      "Step 81000, Avg loss (last 1000): 9.3345\n",
      "Step 82000, Avg loss (last 1000): 10.3146\n",
      "Step 83000, Avg loss (last 1000): 12.1341\n",
      "Step 84000, Avg loss (last 1000): 10.6975\n",
      "Step 85000, Avg loss (last 1000): 10.5979\n",
      "Step 86000, Avg loss (last 1000): 11.1472\n",
      "Step 87000, Avg loss (last 1000): 11.3594\n",
      "Step 88000, Avg loss (last 1000): 11.0497\n",
      "Step 89000, Avg loss (last 1000): 11.5733\n",
      "Step 90000, Avg loss (last 1000): 10.4868\n",
      "Step 91000, Avg loss (last 1000): 10.6448\n",
      "Step 92000, Avg loss (last 1000): 10.3511\n",
      "Step 93000, Avg loss (last 1000): 10.2127\n",
      "Step 94000, Avg loss (last 1000): 10.4427\n",
      "Step 95000, Avg loss (last 1000): 10.5198\n",
      "Step 96000, Avg loss (last 1000): 11.3747\n",
      "Step 97000, Avg loss (last 1000): 11.1310\n",
      "Step 98000, Avg loss (last 1000): 10.5365\n",
      "Step 99000, Avg loss (last 1000): 9.9814\n",
      "Step 100000, Avg loss (last 1000): 9.9602\n",
      "Step 101000, Avg loss (last 1000): 9.7865\n",
      "Step 102000, Avg loss (last 1000): 9.8731\n",
      "Step 103000, Avg loss (last 1000): 9.7114\n",
      "Step 104000, Avg loss (last 1000): 9.3525\n",
      "Step 105000, Avg loss (last 1000): 9.2038\n",
      "Step 106000, Avg loss (last 1000): 9.4334\n",
      "Step 107000, Avg loss (last 1000): 9.2247\n",
      "Step 108000, Avg loss (last 1000): 9.1214\n",
      "Step 109000, Avg loss (last 1000): 8.6511\n",
      "Step 110000, Avg loss (last 1000): 8.5613\n",
      "Step 111000, Avg loss (last 1000): 7.9686\n",
      "Step 112000, Avg loss (last 1000): 8.5576\n",
      "Step 113000, Avg loss (last 1000): 8.2248\n",
      "Step 114000, Avg loss (last 1000): 8.2149\n",
      "Step 115000, Avg loss (last 1000): 8.7559\n",
      "Step 116000, Avg loss (last 1000): 7.8845\n",
      "Step 117000, Avg loss (last 1000): 7.8762\n",
      "Step 118000, Avg loss (last 1000): 8.5372\n",
      "Step 119000, Avg loss (last 1000): 8.0983\n",
      "Step 120000, Avg loss (last 1000): 8.5107\n",
      "Step 121000, Avg loss (last 1000): 8.2644\n",
      "Step 122000, Avg loss (last 1000): 7.6977\n",
      "Step 123000, Avg loss (last 1000): 7.7996\n",
      "Step 124000, Avg loss (last 1000): 8.0002\n",
      "Step 125000, Avg loss (last 1000): 8.0963\n",
      "Step 126000, Avg loss (last 1000): 8.4409\n",
      "Step 127000, Avg loss (last 1000): 8.3044\n",
      "Step 128000, Avg loss (last 1000): 7.7874\n",
      "Step 129000, Avg loss (last 1000): 8.2839\n",
      "Step 130000, Avg loss (last 1000): 7.8309\n",
      "Step 131000, Avg loss (last 1000): 8.1517\n",
      "Step 132000, Avg loss (last 1000): 7.9399\n",
      "Step 133000, Avg loss (last 1000): 7.9919\n",
      "Step 134000, Avg loss (last 1000): 7.4344\n",
      "Step 135000, Avg loss (last 1000): 8.7034\n",
      "Step 136000, Avg loss (last 1000): 7.9858\n",
      "Step 137000, Avg loss (last 1000): 11.2111\n",
      "Step 138000, Avg loss (last 1000): 12.8924\n",
      "Step 139000, Avg loss (last 1000): 11.9554\n",
      "Step 140000, Avg loss (last 1000): 11.3902\n",
      "Step 141000, Avg loss (last 1000): 11.9062\n",
      "Step 142000, Avg loss (last 1000): 11.9592\n",
      "Step 143000, Avg loss (last 1000): 12.3977\n",
      "Step 144000, Avg loss (last 1000): 12.1791\n",
      "Step 145000, Avg loss (last 1000): 11.1533\n",
      "Step 146000, Avg loss (last 1000): 10.9900\n",
      "Step 147000, Avg loss (last 1000): 10.4293\n",
      "Step 148000, Avg loss (last 1000): 10.5243\n",
      "Step 149000, Avg loss (last 1000): 10.0341\n",
      "Step 150000, Avg loss (last 1000): 11.6165\n",
      "Step 151000, Avg loss (last 1000): 10.2536\n",
      "Step 152000, Avg loss (last 1000): 11.2281\n",
      "Step 153000, Avg loss (last 1000): 10.5192\n",
      "Step 154000, Avg loss (last 1000): 10.7324\n",
      "Step 155000, Avg loss (last 1000): 10.6826\n",
      "Step 156000, Avg loss (last 1000): 10.2219\n",
      "Step 157000, Avg loss (last 1000): 11.1049\n",
      "Step 158000, Avg loss (last 1000): 11.4197\n",
      "Step 159000, Avg loss (last 1000): 10.5813\n",
      "Step 160000, Avg loss (last 1000): 10.1291\n",
      "Step 161000, Avg loss (last 1000): 10.5295\n",
      "Step 162000, Avg loss (last 1000): 10.1119\n",
      "Step 163000, Avg loss (last 1000): 9.3209\n",
      "Step 164000, Avg loss (last 1000): 9.3240\n",
      "Step 165000, Avg loss (last 1000): 9.4841\n",
      "Step 166000, Avg loss (last 1000): 10.2142\n",
      "Step 167000, Avg loss (last 1000): 10.9405\n",
      "Step 168000, Avg loss (last 1000): 11.2324\n",
      "Step 169000, Avg loss (last 1000): 10.9312\n",
      "Step 170000, Avg loss (last 1000): 11.1312\n",
      "Step 171000, Avg loss (last 1000): 11.1004\n",
      "Step 172000, Avg loss (last 1000): 9.1708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, target)\n\u001b[1;32m     25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m log_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.12/site-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         state_steps,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     adam(\n\u001b[1;32m    227\u001b[0m         params_with_grad,\n\u001b[1;32m    228\u001b[0m         grads,\n\u001b[1;32m    229\u001b[0m         exp_avgs,\n\u001b[1;32m    230\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    231\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    232\u001b[0m         state_steps,\n\u001b[1;32m    233\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    234\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    235\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    236\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    237\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    238\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    239\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    240\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    241\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    242\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    243\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    244\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    245\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    246\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.12/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.12/site-packages/torch/optim/adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 766\u001b[0m func(\n\u001b[1;32m    767\u001b[0m     params,\n\u001b[1;32m    768\u001b[0m     grads,\n\u001b[1;32m    769\u001b[0m     exp_avgs,\n\u001b[1;32m    770\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    771\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    772\u001b[0m     state_steps,\n\u001b[1;32m    773\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    774\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    775\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    776\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    777\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    778\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    779\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    780\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    781\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    782\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    783\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    784\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    785\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.12/site-packages/torch/optim/adam.py:379\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    376\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m    380\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.12/site-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not load_state:\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    # loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(embedding_layer.parameters(), lr=1e-3)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(1):\n",
    "        \n",
    "        step = 0\n",
    "        log_loss = 0\n",
    "        pairs = build_cbow_pairs(data, context_length)\n",
    "        # print(\"Steps: \", sum(1 for _ in pairs))\n",
    "        for context, target in pairs:\n",
    "            # print(context, \"->\", target)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = embedding_layer(context)\n",
    "\n",
    "            # print(f\"{logits=}\\n{logits.size()=}\\n\")\n",
    "            # print(f\"{target=}\\n{target.size()=}\\n\")\n",
    "            \n",
    "            loss = loss_fn(logits, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            log_loss += loss.item()\n",
    "\n",
    "                if step % 1000 == 0 and step > 0:\n",
    "                    print(f\"Step {step}, Avg loss (last {1000}): {log_loss / 1000:.4f}\")\n",
    "                    losses.append(log_loss)\n",
    "                    log_loss = 0        \n",
    "                step += 1\n",
    "            \n",
    "    torch.save(embedding_layer.state_dict(), 'word_embedding_2.pth')\n",
    "            \n",
    "    torch.save(embedding_layer.state_dict(), 'word_embedding_2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(losses))], losses, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage loss per 1000 steps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss (Cross-Entropy)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([i for i in range(len(losses))], losses, marker='o', linestyle='-')\n",
    "plt.xlabel('Average loss per 1000 steps')\n",
    "plt.ylabel('Loss (Cross-Entropy)')\n",
    "plt.save(\"loss_2.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
